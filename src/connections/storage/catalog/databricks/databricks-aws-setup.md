---
title: Databricks AWS Setup

---

Use the Databricks destination to ingest event data from Segment into the bronze layer of your Delta Lake hosted in Databricks/AWS (S3).

This page will help you connect the Databricks Destination with AWS (S3).

## Prerequisites

Please note the following prerequisites for setup.

1. The target Databricks workspace must be Unity Catalog enabled. Segment doesn't support the Hive metastore. Visit the [Databricks guide](https://docs.databricks.com/en/data-governance/unity-catalog/enable-workspaces.html){:target="_blank"} for enabling the Unity Catalog for more information. 
2. The user or system completing the setup needs the following permissions:
- AWS: The ability to create an S3 bucket and IAM role.
- Databricks: Admin access at the account and workspace level.

## Authentication

Segment supports both OAuth and personal access token (PAT) for authentication. Segment strongly recommends using OAuth as it's easier to set up and manage. Throughout this guide, some instructions are marked as (OAuth only) or (PAT only). You may skip any instructions that don't correspond with your desired authentication method.

## Key terms

As you go through the setup instructions below, keep the following key definititions in mind. 
1. **Databricks Workspace URL**: The base URL for your Databricks workspace.
2. **Service principal Application ID**: The ID tied to the service principal you'll create for Segment. 
3. **Service Principal Secret/Token**: The client secret or PAT you'll create for the service principal. 
4. **Target Unity Catalog**: The catalog where Segment lands your data. 
5. **Workspace Admin Token** (PAT only): The access token you'll generate for your Databricks workspace admin.

## Setup

1. **Find your Databricks Workspace URL**: This is used by you and Segment to access your workspace API. 
- Check your browser's address bar when inside the workspace. The workspace URL will look something like: `https://<workspace-deployment-name>.cloud.databricks.com`. Remove any characters after this portion and note its value for later use.
2. **Create a service principal**. This is used by Segment to access your Databricks workspace and associated APIs.
    1. Follow the [Databricks guide](https://docs.databricks.com/en/administration-guide/users-groups/service-principals.html#manage-service-principals-in-your-account){:target="_blank"} for adding a service principal to your account and assigning to the workspace. This name can be anything, but Segment recommends something that identifies the purpose (for example, `Segment Storage Destinations`). Note the Application ID generated by Databricks for later use. Segment doesn't require Account admin or Marketplace admin roles.
    2. (OAuth only) Follow the [Databricks guide](https://docs.databricks.com/en/dev-tools/authentication-oauth.html#step-2-create-an-oauth-secret-for-a-service-principal){:target="_blank"} for generating an OAuth secret. Note the SEcret generated by Databricks for later use. Once you navigate away from the page the SEcret will no longer be visible. If you lose or forget the Secret you can delete the existing Secret and create a new one. 
3. **Enable entitlements for the service principal on the workspace**: This allows the Segment service principal to create and use a small SQL warehouse which is used for creating and updating table schemas in the Unity Catalog.
    1. Follow the [Databricks guide](https://docs.databricks.com/en/administration-guide/users-groups/service-principals.html#manage-workspace-entitlements-for-a-service-principal){:target="_blank"} for managing workspace entitlements for a service principal. Segment requires the `Allow cluster creation` and `Databricks SQL access` entitlements.
4. **Create an external location and storage credentials**: This will be the storage location where Segment lands your Delta lake and the associated credentials Segment uses to access the storage. 
    1. Follow the [Databricks guide](https://docs.databricks.com/en/data-governance/unity-catalog/manage-external-locations-and-credentials.html){:target="_blank"} for managing external locations and storage credentials. This guide assumes the target S3 bucket already exists. If not, follow the [AWS guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html){:target="_blank"} for creating a bucket.
    2. Once the external location nand storage credentials are created in your Databricks workspace, update the permissions to allow access to the Segment service principal. 
        - In your workspace, navigate to **Data > External Data > Storage Credentials**. Click the name of the credentials created above to go to the Permissions tab. Click **Grant**, then select the Segment service principal from the drop-down. Select the **CREATE EXTERNAL TABLE**, **READ FILES**, and **WRITE FILES** checkboxes. Click **Grant**. 
        - Click **External Locations**. Click the name of the location created above and go to the Permissions tab. Click **Grant**, then select the Segment service principal from the drop-down. Select the **CREATE EXTERNAL TABLE**, **READ FILES**, and **WRITE FILES** checkboxes. Click **Grant**.
    3. In AWS, supplement the Trust policy for the role created when setting up the storage credentials. 
        - Add: `arn:aws:iam::595280932656:role/segment-storage-destinations-production-access` to the Principal list.
        - Convert the `sts:ExternalID` field to a list and add the Segment Workspace ID.
        - You'll find the Segment workspace ID in the Segment app (**Settings > Workspace settings > ID**).
        - The Trust policy should look like:
        ```
        {
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
"arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL",
"arn:aws:iam::<YOUR-AWS-ACCOUNT-ID>:role/<THIS-ROLE-NAME>",
"arn:aws:iam::595280932656:role/segment-storage-destinations-production-access"
         ]
      },
      "Action": "sts:AssumeRole",
      "Condition": {
        "StringEquals": {
          "sts:ExternalId": [
"<DATABRICKS-ACCOUNT-ID>",
"<SEGMENT-WORKSPACE-ID>"
    ]
        }
      }
    }
  ]
}


        ```

5. **Create a workspace admin access token** (PAT only): This is used by your Databricks workspace admin to generate a personal access token for the service principal. 
    1. Follow the [Databricks guide](https://docs.databricks.com/en/dev-tools/auth.html#databricks-personal-access-tokens-for-workspace-users){:target="_blank"} for generating personal access tokens for workspace users. Note the gnerated token for later use.

6. **Enable personal access tokens for the workspace** (PAT only): This allows the creation and use of personal access tokens for the workspace admin and the service principal.
    1. Follow the [Databricks guide](https://docs.databricks.com/en/administration-guide/access-control/tokens.html#enable-or-disable-personal-access-token-authentication-for-the-workspace){:target="_blank"} for enabling personal access token authentication for the workspace. 
    2. Follow the [Databricks guide](https://docs.databricks.com/en/security/auth-authz/api-access-permissions.html#manage-token-permissions-using-the-admin-settings-page){:target="_blank"} to grant Can Use permission to the Segment service principal created earlier. 
7. **Generate a personal access token for the service principal** (PAT only): This is the token used by Segment to access the Databricks workspace API. The Databricks UI doesn't allow for the creation of service principal tokens. Tokens must be generated using either the Databricks workspace API (recommended) or the Databricks CLI. 
    1. Generating a token requires the following values:
    - **Databricks Workspace URL**: The base URL to your Databricks workspace. 
    - **Workspace Admin Token**: The token generated for your Databricks admin user.
    - **Service Principal Application ID**: The ID generated for the Segment service principal. 
    - **Lifetime Seconds**: The number of seconds before the token expires. Note that Segment doesn't prescribe a specific token lifetime. A new token will need to be generated using the process below and updated on the Segment app before the existing token expires. Segment's general guidance is 90 days (7776000 seconds). 
    - **Comment**: A comment which describes the purpose of the token (for example, Grants Segment access to this workspace until 12/21/2023). 
    2. (Recommended option) To create the token with the API, execute the following command in a terminal or command line took. Be sure to update the placeholders with the relevant details from above. For more information about the API check out the [Databricks API docs](https://docs.databricks.com/api/workspace/tokenmanagement/createobotoken){:target="_blank"}.
    - curl --location '<DATABRICKS_WORKSPACE_URL>/api/2.0/token-management/on-behalf-of/tokens' --header 'Content-Type: application/json' --header 'Authorization: Bearer <WORKSPACE_ADMIN_TOKEN>' --data '{"application_id": "<SERVICE_PRINCIPAL_APPLICATION_ID>", "lifetime_seconds": <LIFETIME_SECONDS>, "comment": "<COMMENT>"}'
    - The response from the API contains a `token_value` field. Note this value for later use. 
    3. (Alternative option) If you prefer to use the Databricks CLI, execute the following command in a terminal or command line tool. Be sure to update the placeholders with the relevant details from above. You will also need to [set up a profile](https://docs.databricks.com/en/dev-tools/cli/databricks-cli-ref.html#databricks-personal-access-token-authentication){:target="_blank"} for the CLI. For more information about the CLI, check out the [Databricks CLI docs](https://docs.databricks.com/en/dev-tools/cli/databricks-cli-ref.html){:target="_blank"}. 
    - databricks token-management create-obo-token <SERVICE_PRINCIPAL_APPLICATION_ID> <LIFETIME_SECONDS> --comment <COMMENT> -p <PROFILE_NAME>
    - The response from the CLI will contain a `token_value` field. Note this value for later use. 
8. **Create a new catalog in Unity Catalog and grant Segment permissions**. This is the target catalog where Segment lands your schemas/tables. 
    1. Follow the [Databricks guide](https://docs.databricks.com/en/data-governance/unity-catalog/create-catalogs.html#create-a-catalog){:target="_blank"} for creating a catalog. 
    - Be sure to select the Storage location created earlier. The Catalog name can be any valid catalog name (for example, "Segment"). Note the catalog name for later use. 
    2. Select the newly-created catalog. Click the Permissions tab, then Grant. Select the Segment service principal from the dropdown, check `ALL PREVELEGES`, then click **Grant**.
9. **Setup the Databricks Delta Lake destination in Segment**: This links a Segment events source to your Databricks workspace/catalog.
    1. Navigate to `https://app.segment.com/<WORKSPACE_SLUG>/destinations/catalog/databricks-delta-lake`. 
    2. Click **Add Destination**, select a source, then click **Next**.
    3. Enter the name for your destination, then click **Create destination**.
    4. Enter the connection settings using the values noted above.


